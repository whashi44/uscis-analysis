{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%rest` not found.\n"
     ]
    }
   ],
   "source": [
    "%rest -f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USCIS i-485 analysis \n",
    "This is a test code to pre process the following websites. \n",
    "\n",
    "https://www.immihelp.com/i-485-tracker/\n",
    "https://www.trackitt.com/usa-immigration-trackers/i485-eb\n",
    "\n",
    "The above website shows the code, let's try to download them.\n",
    "\n",
    "Steps\n",
    "1. Analyze the website and see how the data is stored\n",
    "2. use request and bs4 to download data if possible \n",
    "3. save it in excel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Analyze the website and see how the data is stored \n",
    "\n",
    "Why: This step is important becaues without understanding the structure of the HTML code we would not be able to parse it by beautiful soup. \n",
    "The structure of this website is that the data is stored in the class called \"c-Sticky-table\" and structure is as follows: \n",
    "c-Sticky-table\n",
    "- thead - tr - <th> text </th>\n",
    "- tbody \n",
    "  - tr - <td> text </td>\n",
    "  - tr - <td> text </td> .... \n",
    "\n",
    "Based on the above, it is good to perform the following\n",
    "1. Obtain the header by parsing the html as table>thead>th \n",
    "2. Obtain the data by iterating each tr and collect the td from the text "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web scraping portion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h>Import necessary libraries</h>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard library \n",
    "import requests\n",
    "import csv, re\n",
    "import time \n",
    "import cProfile\n",
    "import pstats\n",
    "\n",
    "# external library \n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare infos\n",
    "base_url = \"https://www.immihelp.com/i-485-tracker/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_page(session, url):\n",
    "    \"\"\"This function takes session and url as an argument and return the last page number of the url. \n",
    "\n",
    "    Args:\n",
    "        session (request.session): session object generated by request\n",
    "        url (string): target url \n",
    "\n",
    "    Returns:\n",
    "        last_page (int): last page number of the table \n",
    "    \"\"\"\n",
    "    soup = get_soup(session, url)\n",
    "    # Find how many pages are there\n",
    "    last_page_url = soup.find(title=\"Last Page\", href=True)\n",
    "    # This will result in \"/i-485-tracker/100/\"\n",
    "    last_page_url = last_page_url['href']\n",
    "    # Remove the leading and trailing \"/\"\n",
    "    last_page_url =last_page_url.strip('/')\n",
    "    # Split by \"/\" delimiter\n",
    "    last_page_url = last_page_url.split('/')\n",
    "    # the last item is the page number\n",
    "    last_page_num = last_page_url[-1]\n",
    "    # It is string, hence convert to integer\n",
    "    last_page_num = int(last_page_num)\n",
    "    print(f\"last page of the website is p.{last_page_num}\")\n",
    "    return last_page_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_header(header_table):\n",
    "    \"\"\"This function takes the target table and return header list \n",
    "\n",
    "    Args:\n",
    "        header_table (table object from bs4): \n",
    "\n",
    "    Returns:\n",
    "        list: header list\n",
    "    \"\"\"\n",
    "    headers = []\n",
    "    # For each html tag \"th\" in the table, append the text inside it. This will obtain the header of the table \n",
    "    for col in header_table.find_all(\"th\"):\n",
    "        headers.append(col.text)\n",
    "    print(\"Obtained the header\")\n",
    "    return headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table(session, url, target):\n",
    "    soup = get_soup(session, url)\n",
    "    table = soup.find(class_=target)\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup(session, url):\n",
    "    response = session.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    # iterate through pages version \n",
    "    with requests.Session() as s:\n",
    "        print(\"Session initialized\") \n",
    "        data =[]\n",
    "        last_page = get_last_page(s, base_url)\n",
    "        \n",
    "        target_table = \"c-Sticky-table\"\n",
    "        table = get_table(s, base_url,target_table)\n",
    "        # Initialize headers list \n",
    "        headers = get_header(table.thead)\n",
    "        # last_page = 3\n",
    "        # For each page \n",
    "        for page in range(1,last_page+1):\n",
    "            if page%10==0:\n",
    "                print(f\"working on page {page}\")\n",
    "            # Initialize body table and raw data list \n",
    "            current_page = base_url + f\"{page}/\"\n",
    "            table = get_table(s,current_page,target_table)\n",
    "            body_table = table.tbody\n",
    "            \n",
    "            # For each row in the body of the table \n",
    "            for row in body_table.find_all(\"tr\"):\n",
    "                t_row ={}\n",
    "                # For each col of the row \n",
    "                for col,header in zip(row.find_all(\"td\"),headers):\n",
    "                    # print(f\"printing column {col.text}\")\n",
    "                    # Store them in dictionary, header is the key and col is the data. make sure to remove \"\\n\" and extra spaces\n",
    "                    t_row[header] = col.text.replace(\"\\n\",\"\").strip()\n",
    "                # print(t_row)\n",
    "                data.append(t_row) \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_csv(data):\n",
    "    keys = data[0].keys()\n",
    "    file_name = 'test.csv'\n",
    "    with open(file_name, 'w', encoding='utf-8', newline=\"\") as data_file:\n",
    "        dict_writer = csv.DictWriter(data_file, keys)\n",
    "        dict_writer.writeheader()\n",
    "        dict_writer.writerows(data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    data = get_data()\n",
    "    to_csv(data)\n",
    "# profile = cProfile.Profile()\n",
    "# profile.runcall(get_data)\n",
    "# ps = pstats.Stats(profile)\n",
    "# ps.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session initialized\n",
      "last page of the website is p.101\n",
      "Obtained the header\n",
      "working on page 10\n",
      "working on page 20\n",
      "working on page 30\n",
      "working on page 40\n",
      "working on page 50\n",
      "working on page 60\n",
      "working on page 70\n",
      "working on page 80\n",
      "working on page 90\n",
      "working on page 100\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ec4fe7585462e156b559df938c0de9ba8517dd13ad1671bf818b1cefee983cf1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('.venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
